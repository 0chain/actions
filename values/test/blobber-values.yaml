serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

zminer: false #Enable to make the deployment for 0miners also set blockWorker URL in config.
affinity:
  blobber:
    storageOptimize: false #Enable this to schedule the blobbers preferrablly over to the nodes with labels bound:high-storage.

blobber:
  blobberCount: 6
  replicaCount: 1

  # namespace: helm
  host: 0chain.net

  minioConfig:
    bucketUrl: s3.amazonaws.com
    accessKeyId: ajkfjasdfsdv
    secretAccessKey: davddcasdc
    bucketName: blobber
    bucketRegion: us-east-2

  config:
    version: "1.0"
    logging:
      level: "debug"
      console: "false" # printing log to console is only supported in development mode
    # for testing
    #  500 MB - 536870912
    #    1 GB - 1073741824
    #    2 GB - 2147483648
    #    3 GB - 3221225472
    #  100 GB - 107374182400
    capacity: "107374182400" # 100 GB bytes total blobber capacity
    readPrice: "0.01"     # token / GB for reading
    writePrice: "0.01"    # token / GB / time_unit for writing
    priceInUsd: "true"
    priceWorkerInHours: "12"
    # the time_unit configured in Storage SC and can be given using
    #
    #     ./zbox sc-config
    #
    # min_lock_demand is value in [0; 1] range; it represents number of tokens the
    # blobber earned even if a user will not read or write something
    # to an allocation; the number of tokens will be calculated by the following
    # formula (regarding the time_unit and allocation duration)
    #
    #     allocation_size * write_price * min_lock_demand
    #
    minLockDemand: "0.1"
    # max_offer_duration restrict long contracts where,
    # in the future, prices can be changed
    maxOfferDuration: "744h" # 31 day
    challengeCompletionTime: "2m" # duration to complete a challenge
    # these timeouts required by blobber to check client pools, perform
    # a task and redeem tokens, it should be big enough
    readLockTimeout: "1m"
    writeLockTimeout: "1m"
    maxFileSize: "10485760" #10MB
    # update_allocations_interval used to refresh known allocation objects from SC
    updateAllocationsInterval: "1m"
    # delegate wallet (must be set)
    delegateWallet: "b7ebf17502288c0ef27edc52423372aab12b595f0aeb55f3960a3bbc6c47a574"
    # min stake allowed, tokens
    minStake: "1.0"
    # max stake allowed, tokens
    maxStake: "100.0"
    # maximum allowed number of stake holders
    numDelegates: "50"
    # service charge of the blobber
    serviceCharge: "0.30"
    # min submit from miners
    minSubmit: "50"
    # min confirmation from sharder
    minConfirmation: "50"
    blockWorker: "http://helm-zdns-01"
    handlers:
      rateLimit: "10" # 10 per second
    serverChain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      owner: "edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759"
      genesisBlock:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"
      signatureScheme: "bls0chain"
    contentrefCleaner:
      frequency: "30"
      tolerance: "3600"
    openconnectionCleaner:
      frequency: "30"
      tolerance: "3600"
    writemarkerRedeem:
      frequency: "10"
      numWorkers: "5"
    readmarkerRedeem:
      frequency: "10"
      numWorkers: "5"
    challengeResponse:
      frequency: "10"
      numWorkers: "5"
      maxRetries: "20"
    pg:
      user: postgres
      password: password
    db:
      automigrate: true # true to automigrate with gorm's Automigration
      name: "blobber_meta"
      user: "blobber_user"
      password: "blobber"
      host: "postgres"
      port: "5432"
    geolocation:
      latitude: "0"
      longitude: "0"
    minio:
      # Enable or disable minio backup service
      start: "false"
      # The frequency at which the worker should look for files, Ex: 3600 means it will run every 3600 seconds
      workerFrequency: "1800" # In Seconds
      # Use SSL for connection or not
      useSsl: "false"
    coldStorage:
      # Minimum file size to be considered for moving to cloud
      minFileSize: "1048576" #in bytes
      # Minimum time for which file is not updated or not used
      fileTimeLimitInHours: "1" # 720 #in hours
      # Number of files to be queried and processed at once
      jobQueryLimit: "100"
      # Capacity filled in bytes after which the cloud backup should start work
      startCapacitySize: "10485760" # 536870912 # 500MB
      # Delete local copy once the file is moved to cloud
      deleteLocalCopy: "true"
      # Delete cloud copy if the file is deleted from the blobber by user/other process
      deleteCloudCopy: "true"
    # integration tests related configurations
    integrationTests:
      # address of the server
      address: "host.docker.internal:15210"
      # lock_interval used by nodes to request server to connect to blockchain
      # after start
      lockInterval: "1s"

  image:
    repository: 0chaindev/blobber
    pullPolicy: Always
    tag: staging

  imagePullSecrets: 
    - name: regcred

  nameOverride: ""

  fullnameOverride: ""

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  env:
    DOCKER: "true"
    DB_NAME: blobber_meta
    DB_USER: blobber_user
    DB_PASSWORD: blobber
    DB_PORT: "5432"

  resources:
    requests:
      cpu: "50m"
      memory: "50Mi"
    limits:
      cpu: "100m"
      memory: "100Mi"

  service:
    type: NodePort
    port: 313
    nodePort: 313

  grpcService:
    type: NodePort
    port: 315
    nodePort: 315

  jobImage:
    repository: 0chaindev/stake
    pullPolicy: Always
    tag: latest

  # nodeSelector: {}

  # tolerations: []

  # affinity: {}

  postgresImage:
    repository: postgres
    tag: 11

  postgresResources:
    requests:
      cpu: "50m"
      memory: "50Mi"
    limits:
      cpu: "100m"
      memory: "100Mi"

  postgresEnv:
    POSTGRES_HOST_AUTH_METHOD: trust

  postgresJobEnv:
    POSTGRES_HOST_AUTH_METHOD: trust
    POSTGRES_PORT: "5432"
    POSTGRES_USER: postgres

  postgresService:
    port: 5432

  persistence:
    enabled: false

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:
    storageClassName: openebs-hostpath
    ## rabbitmq data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    
    ######### PV HostPath ###########
    pvHostPath: "/mnt/kubernetes/"
    
    ####### PVs & PVCs #######
    storageClassNameFiles: blob-files-pv-
    accessModeFilesPv: ReadWriteOnce
    storageFiles: 20Gi

    storageClassNameData: blob-valid-data-pv-
    accessModeDataPv: ReadWriteOnce
    storageData: 30Gi

    storageClassNameTmp: blob-tmp-pv-
    accessModeTmpPv: ReadWriteOnce
    storageTmp: 5Gi

    storageClassNameLog: blob-valid-log-pv-
    accessModeLogPv: ReadWriteOnce
    storageLog: 3Gi

validator:
  validatorCount: 6
  replicaCount: 1

  config:
    version: "1.0"

    # delegate wallet (must be set)
    delegateWallet: "b7ebf17502288c0ef27edc52423372aab12b595f0aeb55f3960a3bbc6c47a574"
    # min stake allowed, tokens
    minStake: "1.0"
    # max stake allowed, tokens
    maxStake: "100.0"
    # maximum allowed number of stake holders
    numDelegates: "50"
    # service charge of related blobber
    serviceCharge: "0.30"

    blockWorker: "http://helm-zdns-01"
    handlers:
      rateLimit: "10" # 10 per second

    logging:
      level: "info"
      console: "false" # printing log to console is only supported in development mode

    serverChain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      owner: "edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759"
      genesisBlock:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"
      network:
        relayTime: "100" # milliseconds
      signatureScheme: "bls0chain"

  image:
    repository: 0chaindev/validator
    pullPolicy: Always
    tag: staging

  imagePullSecrets: 
    - name: regcred

  nameOverride: ""

  fullnameOverride: ""

  env:
    DOCKER: "true"

  # persistence:
  #   enabled: false

  #   ## A manually managed Persistent Volume and Claim
  #   ## Requires persistence.enabled: true
  #   ## If defined, PVC must be created manually before volume will be bound
  #   # existingClaim:

  #   ## rabbitmq data Persistent Volume Storage Class
  #   ## If defined, storageClassName: <storageClass>
  #   ## If set to "-", storageClassName: "", which disables dynamic provisioning
  #   ## If undefined (the default) or set to null, no storageClassName spec is
  #   ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  #   ##   GKE, AWS & OpenStack)
  #   ##
  #   # storageClass: "-"
  #   # accessMode: ReadWriteOnce
  #   # size: 20Gi

  # podAnnotations: {}

  # podSecurityContext: {}
  #   # fsGroup: 2000

  # securityContext: {}
  #   # capabilities:
  #   #   drop:
  #   #   - ALL
  #   # readOnlyRootFilesystem: true
  #   # runAsNonRoot: true
  #   # runAsUser: 1000

  service:
    type: NodePort
    port: 314
    nodePort: 314

  resources:
    requests:
      cpu: "50m"
      memory: "50Mi"
    limits:
      cpu: "100m"
      memory: "100Mi"

  # resources: {}

  # nodeSelector: {}

  # tolerations: []

  # affinity: {}